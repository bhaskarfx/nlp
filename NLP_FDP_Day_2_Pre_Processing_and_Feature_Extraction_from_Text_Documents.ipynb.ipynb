{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Pre-Processing and Feature Extraction from Text Documents.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Processing Raw Text**\n",
        "\n",
        "by Dr. Bhaskar Mondal\n",
        "\n",
        "https://sites.google.com/view/bmondal/ \n",
        "\n",
        "YouTude Link: https://www.youtube.com/watch?v=sQb63jW4wz4"
      ],
      "metadata": {
        "id": "-3jRVlvAfCaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Access text from local files and from the web**"
      ],
      "metadata": {
        "id": "V6wPiLKmfTEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R7eum49e2VW"
      },
      "outputs": [],
      "source": [
        "tw=\"\"\"I luv my &lt;3 iphone &amp; you're awsm apple. \n",
        "DisplayIsAwesome, sooo happppppy 🙂 http://www.apple.com\"\"\" "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load text \n",
        "# filename = 'metamorphosis_clean.txt'\n",
        "# file = open(filename, 'rt')\n",
        "# text = file.read()\n",
        "# file.close()"
      ],
      "metadata": {
        "id": "UeYe4P6Rst1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text=\"\"\"The cloud no-9 :  in the sky is PinkishBblue e . You shouldn't eat cardboard.\n",
        "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\"\"\""
      ],
      "metadata": {
        "id": "pJ0opg8itKvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoding the text**"
      ],
      "metadata": {
        "id": "1BisKCfliIj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tw_decoded = tw.encode(\"ascii\",\"ignore\")\n",
        "tw_decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXZ3_7LOiGD-",
        "outputId": "f6e9bf15-0b78-48f5-f565-3267aad2b11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"I luv my &lt;3 iphone &amp; you're awsm apple. \\nDisplayIsAwesome, sooo happppppy  http://www.apple.com\""
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tw_decoded = tw.encode(\"ascii\",\"ignore\").decode(\"utf8\")\n",
        "tw_decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4-GtwPflkQJf",
        "outputId": "4e581a2f-6e58-4c7c-ded9-27855ceaba37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I luv my &lt;3 iphone &amp; you're awsm apple. \\nDisplayIsAwesome, sooo happppppy  http://www.apple.com\""
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split by Whitespace**"
      ],
      "metadata": {
        "id": "B77N8xuEs9Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "print(words[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EObs7nis-V4",
        "outputId": "6704bf56-dd67-43a0-955c-85af8d96c9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cloud', 'no-9', ':', 'in', 'the', 'sky', 'is', 'PinkishBblue', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split based on words only\n",
        "import re\n",
        "wordst = re.split(r'\\W+', text)\n",
        "print(wordst[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m962ZvnLwBU9",
        "outputId": "770d3ede-5b89-4db8-8939-c3fd47bf2771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cloud', 'no', '9', 'in', 'the', 'sky', 'is', 'PinkishBblue', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Words and characters**"
      ],
      "metadata": {
        "id": "coH7XtuYUpPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of Words\n",
        "word_count = len(str(text).split(\" \"))\n",
        "word_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sgMg1c6Uvqc",
        "outputId": "1bb4595e-36e8-496b-f904-b43bc69e8491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of Words\n",
        "char_count = len(text)\n",
        "char_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dkj_B9P4VmjC",
        "outputId": "18093a95-9551-481e-9c8e-bd7bcf64eec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "162"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use Regular Expressions for cleaning text**\n",
        "\n"
      ],
      "metadata": {
        "id": "zAyqDlv3lRRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split attached words\n",
        "import re\n",
        "cleaned = \" \".join(re.findall(\"[A-Z][^A-Z]*\", tw_decoded))\n",
        "cleaned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H6HVW2VDkwjc",
        "outputId": "5a031cf1-9eb8-4aa4-b720-e7dd61a01d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I luv my &lt;3 iphone &amp; you're awsm apple. \\n Display Is Awesome, sooo happppppy  http://www.apple.com\""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "APPOSTOPHES= {\"'s\": \" is\", \"'re\":\" are\"}    ## Need a huge dictionary\n",
        "# split the words based on whitespace\n",
        "sentence_list = cleaned.split()\n",
        "# make a place where we can build our new sentence\n",
        "new_sentence = []\n",
        "# look through each word \n",
        "for word in sentence_list:\n",
        "    # look for each candidate\n",
        "    for candidate_replacement in APPOSTOPHES:\n",
        "        # if our candidate is there in the word\n",
        "        if candidate_replacement in word:\n",
        "            # replace it \n",
        "            word = word.replace(candidate_replacement, APPOSTOPHES[candidate_replacement])\n",
        "    # and pop it onto a new list \n",
        "    new_sentence.append(word)\n",
        "rfrm = \" \".join(new_sentence)\n",
        "print(rfrm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS6AYOL3mOg5",
        "outputId": "e9dc1b70-a5ba-475d-c6d3-de6c3c541d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I luv my &lt;3 iphone &amp; you are awsm apple. Display Is Awesome, sooo happppppy http://www.apple.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Apostrophe**"
      ],
      "metadata": {
        "id": "sqty1kJnHv0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "\n",
        "print(decontracted(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqZvJx-pH6mt",
        "outputId": "dc03e3ad-d53b-42b7-df5e-a20cce0d2626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cloud no-9 :  in the sky is PinkishBblue e . You should not eat cardboard.\n",
            "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhP8iLRnYRHk",
        "outputId": "e61d9bbd-0371-4a7b-cd65-252d39e57d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 57.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85445 sha256=430bbcddb405cca022f99aa5d5432c06472c90af94ce55168cc59544fe28640e\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import contractions\n",
        "\n",
        "# creating an empty list\n",
        "expanded_words = []    \n",
        "for word in text.split():\n",
        "  # using contractions.fix to expand the shotened words\n",
        "  expanded_words.append(contractions.fix(word))   \n",
        "    \n",
        "expanded_text = ' '.join(expanded_words)\n",
        "\n",
        "print('Expanded_text: ' + expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V68zfH5HYTaL",
        "outputId": "e12d335f-6de0-4968-9ff6-ee0db581a563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanded_text: The cloud no-9 : in the sky is PinkishBblue e . You should not eat cardboard. Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average Word Length**"
      ],
      "metadata": {
        "id": "GzE0OKa4WozW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Word Length\n",
        "def avg_word_len(sentence):\n",
        "  words = sentence.split()\n",
        "  total_len=sum(len(word) for word in words)\n",
        "  no_of_words=len(words)\n",
        "  return (total_len/no_of_words)\n",
        "\n",
        "avg_w_len = avg_word_len(text)\n",
        "avg_w_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDajEkPkWnUJ",
        "outputId": "09aee835-2233-469e-a404-6ecf98fd4f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.225806451612903"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_string = ' '.join([w for w in text.split() if len(w)>1])\n",
        "new_string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-1VRHo5cZHmq",
        "outputId": "ac247f68-a854-4884-c3be-2bf2cbb0aa14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The cloud no-9 in the sky is PinkishBblue You shouldn't eat cardboard. Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\""
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_single_characters(data):\n",
        "  new_text = \"\"\n",
        "  data=data.split()\n",
        "  for w in data:\n",
        "    if len(w) > 1:\n",
        "      print(w)\n",
        "      new_text = new_text + \" \" + w\n",
        "  return new_text\n",
        "\n",
        "print(remove_single_characters(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqFVqxjiWzHJ",
        "outputId": "395ccd0e-7576-47dc-e5a3-9371f0f2852f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "cloud\n",
            "no-9\n",
            "in\n",
            "the\n",
            "sky\n",
            "is\n",
            "PinkishBblue\n",
            "You\n",
            "shouldn't\n",
            "eat\n",
            "cardboard.\n",
            "Hello\n",
            "Mr.\n",
            "Smith,\n",
            "how\n",
            "are\n",
            "you\n",
            "doing\n",
            "today?\n",
            "The\n",
            "weather\n",
            "is\n",
            "great,\n",
            "and\n",
            "city\n",
            "is\n",
            "awesome.\n",
            " The cloud no-9 in the sky is PinkishBblue You shouldn't eat cardboard. Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizing Case:** **convert to lower case** "
      ],
      "metadata": {
        "id": "5qFL0Nprx3uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to lower case\n",
        "def convert_lower_case(words):\n",
        "  words = [word.lower() for word in words]\n",
        "  return words\n",
        "\n",
        "print(convert_lower_case(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0hEWz9qx6kO",
        "outputId": "e3a5a6bd-72cf-4496-9ee6-86dbccd43c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'cloud', 'no-9', ':', 'in', 'the', 'sky', 'is', 'pinkishbblue', 'e', '.', 'you', \"shouldn't\", 'eat', 'cardboard.', 'hello', 'mr.', 'smith,', 'how', 'are', 'you', 'doing', 'today?', 'the', 'weather', 'is', 'great,', 'and', 'city', 'is', 'awesome.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Stop Words**"
      ],
      "metadata": {
        "id": "XkUfoVKW8YI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqmdKyRJ8gTd",
        "outputId": "14896678-2ba2-4cca-a915-1ca121a7f367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VPU5LB68Y5u",
        "outputId": "91a2efd7-759d-48eb-b4c3-36e3b38b5efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of stopwords\n",
        "def no_of_stop_word(sentence):\n",
        "  words = sentence.split()\n",
        "  stops=[]\n",
        "  for word in words:\n",
        "    if word in stop_words:\n",
        "      # print(word)\n",
        "      stops.append(word)\n",
        "  return len(stops)\n",
        "\n",
        "no_of_stop=no_of_stop_word(text)\n",
        "no_of_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blUU1teDXtEP",
        "outputId": "a5792fc8-3df5-4b88-bd43-6686c386ae3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w for w in words if not w in stop_words]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foLsj44n9Jm7",
        "outputId": "1b4ace87-5343-416b-e3e2-fa8783064889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cloud', 'no-9', ':', 'sky', 'PinkishBblue', 'e', '.', 'You', 'eat', 'cardboard.', 'Hello', 'Mr.', 'Smith,', 'today?', 'The', 'weather', 'great,', 'city', 'awesome.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(lower_tokens):\n",
        "    filtered_words=[]\n",
        "    for word in lower_tokens:\n",
        "      if word not in stop_words:\n",
        "        filtered_words.append(word)\n",
        "    return filtered_words\n",
        "\n",
        "print(remove_stopwords(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAv564-KJ_CI",
        "outputId": "ad5cd892-d844-4c47-a6e3-188af0394d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cloud', 'no-9', ':', 'sky', 'PinkishBblue', 'e', '.', 'You', 'eat', 'cardboard.', 'Hello', 'Mr.', 'Smith,', 'today?', 'The', 'weather', 'great,', 'city', 'awesome.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Punctuation**"
      ],
      "metadata": {
        "id": "-4NfN-fs8QhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdzbajv9wXMx",
        "outputId": "e1420bff-6f8d-44f8-ae6b-6f0b8e00f682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "def remove_punct(text):\n",
        "    text_nopunct = ''\n",
        "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
        "    return text_nopunct\n",
        "\n",
        "\n",
        "punc_removed=remove_punct(text)\n",
        "print(punc_removed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1VTXMxlwiS5",
        "outputId": "5316a2bb-5a34-4b25-b951-17d245d8be0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cloud no9   in the sky is PinkishBblue e  You shouldnt eat cardboard\n",
            "Hello Mr Smith how are you doing today The weather is great and city is awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(words):\n",
        "    table=str.maketrans('', '', string.punctuation)\n",
        "    stripped=[w.translate(table) for w in words]\n",
        "    return stripped\n",
        "\n",
        "\n",
        "punc_removed=remove_punct(text)\n",
        "print(punc_removed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkPmwZ0lYvZ4",
        "outputId": "2f303c82-92f8-4ab1-ef54-ac2082e79cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cloud no9   in the sky is PinkishBblue e  You shouldnt eat cardboard\n",
            "Hello Mr Smith how are you doing today The weather is great and city is awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of numerics**"
      ],
      "metadata": {
        "id": "C56IdIUWb5JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of numerics\n",
        "def count_of_nums(sentence):\n",
        "  words = sentence.split()\n",
        "  stops=[]\n",
        "  count=0\n",
        "  for word in words:\n",
        "    if word.isdigit():\n",
        "      count+=1\n",
        "  return count\n",
        "\n",
        "no_of_nums=count_of_nums(text)\n",
        "no_of_nums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-FCMc_Nb5cn",
        "outputId": "bc010951-cf9e-4de3-84f1-6f9e46df33ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEMFRBr8Ls0y",
        "outputId": "4338292d-2cde-46f8-f06b-c461d9c927c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▎                            | 10 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 101 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove numerics \n",
        "def remove_nums(words):\n",
        "  new_string = ''.join((x for x in words if not x.isdigit()))\n",
        "  return new_string\n",
        "\n",
        "print(remove_nums(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIBmettILUJk",
        "outputId": "e2e0ad2c-d3e7-4cce-97be-add2f84a583a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cloud no- :  in the sky is PinkishBblue e . You shouldn't eat cardboard.\n",
            "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of numerics \n",
        "def nums_to_word(words):\n",
        "  import num2words\n",
        "  temp=[]\n",
        "  words = words.split()\n",
        "  for word in words:\n",
        "    if word.isdigit():\n",
        "      temp.append(num2words.num2words(word))\n",
        "    else:\n",
        "      temp.append(word)\n",
        "  return temp\n",
        "\n",
        "\n",
        "print(nums_to_word(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFXdVxNdOBez",
        "outputId": "3357ebee-63ea-47d0-e260-950ba40fe0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cloud', 'no-9', ':', 'in', 'the', 'sky', 'is', 'PinkishBblue', 'e', '.', 'You', \"shouldn't\", 'eat', 'cardboard.', 'Hello', 'Mr.', 'Smith,', 'how', 'are', 'you', 'doing', 'today?', 'The', 'weather', 'is', 'great,', 'and', 'city', 'is', 'awesome.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**"
      ],
      "metadata": {
        "id": "5493ZFydgUpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split into Sentences**"
      ],
      "metadata": {
        "id": "Ggy5S4w55--0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n7bNVrU6GJ-",
        "outputId": "60f1ad06-64dd-46e8-c9b6-cff44ba99760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "g3FDRiIi52_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460266e4-d3c9-438e-fd3a-d94b526af807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The cloud no-9 :  in the sky is PinkishBblue e .', \"You shouldn't eat cardboard.\", 'Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into words\n",
        "def tokenization(sentence):\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  return (word_tokenize(sentence))\n",
        "\n",
        "tokens=tokenization(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "yVz6YHqh7Irk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c384c3-70bd-4205-f445-779c3c0b6198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cloud', 'no-9', ':', 'in', 'the', 'sky', 'is', 'PinkishBblue', 'e', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.', 'Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "TextBlob(text).words"
      ],
      "metadata": {
        "id": "11Du1PAggmqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efc9458-4923-4fe4-a2bf-d1340be00f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['The', 'cloud', 'no-9', 'in', 'the', 'sky', 'is', 'PinkishBblue', 'e', 'You', 'should', \"n't\", 'eat', 'cardboard', 'Hello', 'Mr', 'Smith', 'how', 'are', 'you', 'doing', 'today', 'The', 'weather', 'is', 'great', 'and', 'city', 'is', 'awesome'])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spelling correction**"
      ],
      "metadata": {
        "id": "fBhXdZW7dryL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Spelling correction\n",
        "def spell_correct(sentence):\n",
        "  from textblob import TextBlob\n",
        "  sentence = TextBlob(sentence).correct()\n",
        "  return sentence\n",
        "\n",
        "no_of_nums=spell_correct(text)\n",
        "no_of_nums"
      ],
      "metadata": {
        "id": "eUH9WODjdsiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0fa94f-b00c-406d-b7c7-2e7741fe2a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"The cloud no-9 :  in the sky is PinkishBblue e . You shouldn't eat cardboard.\n",
              "Hello Or. Smith, how are you doing today? The weather is great, and city is awesome.\")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming and Lemmatization**"
      ],
      "metadata": {
        "id": "YyiQAdFaTZ77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "\n",
        "#proide a word to be stemmed\n",
        "print(\"Porter Stemmer\")\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"trouble\"))\n",
        "print(\"Lancaster Stemmer\") \n",
        "print(lancaster.stem(\"cats\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sww6cHykTaJi",
        "outputId": "d53e7fca-a3c3-4214-a64d-4efe98623bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer\n",
            "cat\n",
            "troubl\n",
            "Lancaster Stemmer\n",
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stem Words**"
      ],
      "metadata": {
        "id": "MUVuvcVZ9kjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming of words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in words]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQOZreKZ9lvh",
        "outputId": "b36935df-f9c2-4913-c582-bfd57325a231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'cloud', 'no-9', ':', 'sky', 'pinkishbblu', 'e', '.', 'you', 'eat', 'cardboard.', 'hello', 'mr.', 'smith,', 'today?', 'the', 'weather', 'great,', 'citi', 'awesome.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization** "
      ],
      "metadata": {
        "id": "KVOtLyz_-95-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "def lemmatization(sentence):\n",
        "  words = sentence.split()\n",
        "  lemmas=[]\n",
        "  for word in words:\n",
        "      lemmas.append(Word(word).lemmatize())\n",
        "  return lemmas\n",
        "\n",
        "lemmas=lemmatization(text)\n",
        "lemmas"
      ],
      "metadata": {
        "id": "gtId6-va_BSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e217ad97-7a1e-4de7-fc24-2acda212b487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'cloud',\n",
              " 'no-9',\n",
              " ':',\n",
              " 'in',\n",
              " 'the',\n",
              " 'sky',\n",
              " 'is',\n",
              " 'PinkishBblue',\n",
              " 'e',\n",
              " '.',\n",
              " 'You',\n",
              " \"shouldn't\",\n",
              " 'eat',\n",
              " 'cardboard.',\n",
              " 'Hello',\n",
              " 'Mr.',\n",
              " 'Smith,',\n",
              " 'how',\n",
              " 'are',\n",
              " 'you',\n",
              " 'doing',\n",
              " 'today?',\n",
              " 'The',\n",
              " 'weather',\n",
              " 'is',\n",
              " 'great,',\n",
              " 'and',\n",
              " 'city',\n",
              " 'is',\n",
              " 'awesome.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of speech tagging (POS)**"
      ],
      "metadata": {
        "id": "JlP1zH_0EVxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrnwYT93Y30M",
        "outputId": "5a33ec43-06a8-43b5-b93a-ac0bf64b1720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
        "from textblob import TextBlob\n",
        "result = TextBlob(input_str)\n",
        "print(result.tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNik-6rIXuPT",
        "outputId": "b8299124-d182-42ac-c73a-227ed2ca200b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking using NLTK:**\n",
        "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.)"
      ],
      "metadata": {
        "id": "xd8XcJXvYPwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text = \"learn php from guru99\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "tag = nltk.pos_tag(tokens)\n",
        "print(tag)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp  =nltk.RegexpParser(grammar)\n",
        "result = cp.parse(tag)\n",
        "print(result)\n",
        "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
      ],
      "metadata": {
        "id": "4y9nAga7ALjC",
        "outputId": "5cd6c777-28e4-43a9-9867-ac3d7a0354df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['learn', 'php', 'from', 'guru99']\n",
            "[('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN')]\n",
            "(S (NP learn/JJ php/NN) from/IN (NP guru99/NN))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-e653afdf352a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# It will draw the pattern graphically which can be seen in Noun Phrase chunking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \"\"\"\n\u001b[0;32m--> 863\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-x>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "rp = nltk.RegexpParser(reg_exp)\n",
        "result = rp.parse(result.tags)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6PLlyFkZKPz",
        "outputId": "d10ba54e-892b-4fbc-a420-90a81b5a278c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Parts/NNS\n",
            "  of/IN\n",
            "  (NP speech/NN)\n",
            "  examples/NNS\n",
            "  (NP an/DT article/NN)\n",
            "  to/TO\n",
            "  write/VB\n",
            "  interesting/VBG\n",
            "  easily/RB\n",
            "  and/CC\n",
            "  of/IN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named entity recognition**\n",
        "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)."
      ],
      "metadata": {
        "id": "t9nsto9QaOSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import Tree, pos_tag, ne_chunk\n",
        "sentence = \"Michael and John is reading a booklet in a library of Jakarta\"\n",
        "tagged_sent = ne_chunk(pos_tag(sentence.split()))\n",
        "tagged_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "60dpuDA1aLf8",
        "outputId": "bb0b1208-f4e1-4123-aef9-9013100ab099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('GPE', [('Michael', 'NNP')]), ('and', 'CC'), Tree('PERSON', [('John', 'NNP')]), ('is', 'VBZ'), ('reading', 'VBG'), ('a', 'DT'), ('booklet', 'NN'), ('in', 'IN'), ('a', 'DT'), ('library', 'NN'), ('of', 'IN'), Tree('GPE', [('Jakarta', 'NNP')])])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punc(data)\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_single_characters(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = remove_stopwords(data)\n",
        "    data = stemming(data)\n",
        "    data = convert_numbers(data)"
      ],
      "metadata": {
        "id": "hl_l5NC0EWBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advance Text Processing: N-grams**"
      ],
      "metadata": {
        "id": "adPkhsP5iqtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(text).ngrams(2)"
      ],
      "metadata": {
        "id": "8CpaCpKQirGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64dd9d5c-89cb-44f6-ddea-820ae40a1202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'cloud']),\n",
              " WordList(['cloud', 'no-9']),\n",
              " WordList(['no-9', 'in']),\n",
              " WordList(['in', 'the']),\n",
              " WordList(['the', 'sky']),\n",
              " WordList(['sky', 'is']),\n",
              " WordList(['is', 'PinkishBblue']),\n",
              " WordList(['PinkishBblue', 'e']),\n",
              " WordList(['e', 'You']),\n",
              " WordList(['You', 'should']),\n",
              " WordList(['should', \"n't\"]),\n",
              " WordList([\"n't\", 'eat']),\n",
              " WordList(['eat', 'cardboard']),\n",
              " WordList(['cardboard', 'Hello']),\n",
              " WordList(['Hello', 'Mr']),\n",
              " WordList(['Mr', 'Smith']),\n",
              " WordList(['Smith', 'how']),\n",
              " WordList(['how', 'are']),\n",
              " WordList(['are', 'you']),\n",
              " WordList(['you', 'doing']),\n",
              " WordList(['doing', 'today']),\n",
              " WordList(['today', 'The']),\n",
              " WordList(['The', 'weather']),\n",
              " WordList(['weather', 'is']),\n",
              " WordList(['is', 'great']),\n",
              " WordList(['great', 'and']),\n",
              " WordList(['and', 'city']),\n",
              " WordList(['city', 'is']),\n",
              " WordList(['is', 'awesome'])]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Bag-of-Words (BoW) and TF-IDF for Creating Features from Text**\n",
        "Lets have 3 review:\n",
        "\n",
        "\n",
        "*   Review 1: This movie is very scary and long\n",
        "*   Review 2: This movie is not scary and is slow\n",
        "*   Review 3: This movie is spooky and good\n",
        "\n",
        "The vocabulary consists of these 11 words:\n",
        "\\begin{array}{c|c|c|c|c|c|c|c|c|c|c|c}\\hline\n",
        "&This& movie& is& very& scary& and& long& not&  slow& spooky&  good& Review Length\\\\\\hline\n",
        "Review 1&1&1&1&1&1&1&1&0&0&0&0&7\\\\\\hline\n",
        "Review 2&1&1&2&0&0&1&1&0&1&0&0&8\\\\\\hline\n",
        "Review 3&1&1&1&0&0&0&1&0&0&1&1&6\\\\\\hline\n",
        "\\end{array}"
      ],
      "metadata": {
        "id": "2VjTdCDGwQtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Term Frequency-Inverse Document Frequency**\n",
        "---\n",
        "*   **Term Frequency (TF):** In document d, the frequency represents the number of instances of a given word/ term t.\n",
        "\n",
        ">> $tf(t,d) = \\frac{\\textit{count of t in d} }{ \\textit{number of terms in d}}$\n",
        "\n",
        ">>for example \n",
        "* TF(‘movie’) = 1/8\n",
        "* TF(‘is’) = 2/8 = 1/4\n",
        "\n",
        "*   **Document Frequency (DF):** is the number of total occurrences of the term $t$ in the document set N (entire corpus).\n",
        "\n",
        ">>$df(t) =$ occurrence of $t$ in all documents\n",
        "\n",
        "\n",
        "\n",
        "* **Inverse Document Frequency (IDF):** Log of the ratio numbers of document $N$ and $df(t)$\n",
        "\n",
        ">>$idf(t) = \\log(\\frac{N}{ df(t)})$\n",
        "\n",
        ">>for example \n",
        "* IDF(‘movie’, ) = log(3/3) = 0\n",
        "* IDF(‘is’) = log(3/3) = 0\n",
        "* IDF(‘not’) = log(3/1) = log(3) = 0.48\n",
        "\n",
        "* **Computation of tf-idf weight:** \n",
        ">> tf-idf$(t, d) = tf(t, d) \\times idf(t)$\n",
        "\n",
        ">> for example \n",
        "* TF-IDF(‘this’, Review 2) = TF(‘this’, Review 2) * IDF(‘this’) = 1/8 * 0 = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "8XReeA_yjGpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r1=\" This movie is very scary and long\"\n",
        "r2=\"This movie is not scary and is slow\"\n",
        "r3=\"This movie is spooky and good\"\n",
        "corpus=[r1, r2, r3]\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0p56HDBj7C",
        "outputId": "fa08d36b-63c6-44fb-ddd0-c5eabef46984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' This movie is very scary and long',\n",
              " 'This movie is not scary and is slow',\n",
              " 'This movie is spooky and good']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  preprocessed=[]\n",
        "  \n",
        "  for d in data:\n",
        "    tokens=tokenization(d)\n",
        "    # print(tokens)\n",
        "    tokens = convert_lower_case(tokens)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = remove_punc(tokens)\n",
        "    preprocessed.append(tokens)\n",
        "  return preprocessed\n",
        "\n",
        "print(preprocess(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBr4jKXxfICI",
        "outputId": "135e725b-750b-46b2-b235-d23ab1cac7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['movie', 'scary', 'long'], ['movie', 'scary', 'slow'], ['movie', 'spooky', 'good']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed=preprocess(corpus)\n",
        "print(preprocessed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5N7buDjfqOn",
        "outputId": "8e5f2a59-3a1c-4407-b887-e38495b67d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['movie', 'scary', 'long'], ['movie', 'scary', 'slow'], ['movie', 'spooky', 'good']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a vocabulary list\n",
        "set().union(*preprocessed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFZUSu44fy_k",
        "outputId": "0b9e1f72-d9a6-4e7a-89d4-b5eef17098b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'good', 'long', 'movie', 'scary', 'slow', 'spooky'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_list(ListofList):\n",
        "  list_new=[]\n",
        "  cnt=0\n",
        "  while cnt<len(ListofList):\n",
        "    for i in ListofList[cnt]:\n",
        "        if i in list_new:\n",
        "            continue\n",
        "        else:\n",
        "            list_new.append(i)\n",
        "    cnt+=1\n",
        "  return list_new\n",
        "vocab=vocab_list(preprocessed)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f6BbDAUf9Tk",
        "outputId": "cd549766-36ec-4bc5-c925-6c3937290f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie', 'scary', 'long', 'slow', 'spooky', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Term frequency\n",
        "def tf(corpus):\n",
        "    dic={}\n",
        "    for document in corpus:\n",
        "        for word in document.split():\n",
        "            if word in dic:\n",
        "                dic[word] = dic[word] + 1\n",
        "            else:\n",
        "                dic[word]=1\n",
        "    for word,freq in dic.items():\n",
        "        print(word,freq)\n",
        "        dic[word]=freq/sum(map(len, (document.split() for document in corpus)))\n",
        "    return dic\n",
        "tf=tf(corpus)\n",
        "print(tf)"
      ],
      "metadata": {
        "id": "6n1xuuRdjJmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c952fa-156e-4242-cff1-5d812b9e4c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This 3\n",
            "movie 3\n",
            "is 4\n",
            "very 1\n",
            "scary 2\n",
            "and 3\n",
            "long 1\n",
            "not 1\n",
            "slow 1\n",
            "spooky 1\n",
            "good 1\n",
            "{'This': 0.14285714285714285, 'movie': 0.14285714285714285, 'is': 0.19047619047619047, 'very': 0.047619047619047616, 'scary': 0.09523809523809523, 'and': 0.14285714285714285, 'long': 0.047619047619047616, 'not': 0.047619047619047616, 'slow': 0.047619047619047616, 'spooky': 0.047619047619047616, 'good': 0.047619047619047616}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inverse Document Frequency\n",
        "import math\n",
        "import numpy as np\n",
        "def IDF(corpus, vocab):\n",
        "  idf_dict={}\n",
        "  N=len(corpus)\n",
        "  for i in vocab:\n",
        "    count=0\n",
        "    for sen in corpus:\n",
        "      if i in sen.split():\n",
        "        count=count+1\n",
        "        idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
        "  return idf_dict\n"
      ],
      "metadata": {
        "id": "e6pJOWeyjcXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf=IDF(corpus, vocab)\n",
        "print(idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mfd8Y0YdqNu",
        "outputId": "77600527-6c69-4ae1-fbbb-9ce6eb929bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'movie': 1.0, 'scary': 1.2876820724517808, 'long': 1.6931471805599454, 'slow': 1.6931471805599454, 'spooky': 1.6931471805599454, 'good': 1.6931471805599454}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Term Frequency – Inverse Document Frequency (TF-IDF)\n",
        "tfidf={}\n",
        "\n",
        "res = {key: tf[key] * idf.get(key, 0) \n",
        "                       for key in tf.keys()}"
      ],
      "metadata": {
        "id": "AXoj6sGIjiHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPcdqiy-gHEZ",
        "outputId": "86ea7054-5d65-4107-f736-3b425f83ba78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'This': 0.0,\n",
              " 'and': 0.0,\n",
              " 'good': 0.08062605621714025,\n",
              " 'is': 0.0,\n",
              " 'long': 0.08062605621714025,\n",
              " 'movie': 0.14285714285714285,\n",
              " 'not': 0.0,\n",
              " 'scary': 0.12263638785255054,\n",
              " 'slow': 0.08062605621714025,\n",
              " 'spooky': 0.08062605621714025,\n",
              " 'very': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature extraction: Bag of Words**"
      ],
      "metadata": {
        "id": "lKK_cNg__G2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Bag of Words Algorithm with Python**"
      ],
      "metadata": {
        "id": "XOuoX0pRBgvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_list(ListofList):\n",
        "  list_new=[]\n",
        "  cnt=0\n",
        "  while cnt<len(ListofList):\n",
        "    for i in ListofList[cnt]:\n",
        "        if i in list_new:\n",
        "            continue\n",
        "        else:\n",
        "            list_new.append(i)\n",
        "    cnt+=1\n",
        "  return list_new\n",
        "print(vocab_list(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjdmAP-cp4Bx",
        "outputId": "b4ea0680-dfeb-45b9-ed6e-dcdc4a62f0c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie', 'scary', 'long', 'slow', 'spooky', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens):\n",
        "    ''' This function takes list of words in a sentence as input and returns a vector of size of filtered_vocab.It puts 0 if the word is not present in tokens and count of token if present.'''\n",
        "    vocab=vocab_list(tokens)\n",
        "    print(vocab)\n",
        "    vectors=[]\n",
        "    for t in tokens:\n",
        "      print(t)\n",
        "      vector=[]\n",
        "      for w in vocab:\n",
        "        vector.append(t.count(w))\n",
        "      # print(vector)\n",
        "      vectors.append(vector)\n",
        "    return vectors\n",
        "\n",
        "\n",
        "#create a vocabulary list\n",
        "# vocab=vocab(tokens)\n",
        "# print(vocab)\n",
        "#convert sentences into vectords\n",
        "\n",
        "vectors=vectorize(preprocessed)\n",
        "print(vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYc41z43_Or-",
        "outputId": "58a632f3-541d-4a50-b889-6cd762636366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie', 'scary', 'long', 'slow', 'spooky', 'good']\n",
            "['movie', 'scary', 'long']\n",
            "['movie', 'scary', 'slow']\n",
            "['movie', 'spooky', 'good']\n",
            "[[1, 1, 1, 0, 0, 0], [1, 1, 0, 1, 0, 0], [1, 0, 0, 0, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Bag of Words Model with Sklearn**"
      ],
      "metadata": {
        "id": "9NsehhSOB1CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
        "                           stop_words='english')\n",
        "#transform\n",
        "Count_data = CountVec.fit_transform(corpus)\n",
        " \n",
        "#create dataframe\n",
        "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
        "print(cv_dataframe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4WOuJyYByVl",
        "outputId": "a171484a-50aa-47dc-e883-d862eb89f4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   good  long  movie  scary  slow  spooky\n",
            "0     0     1      1      1     0       0\n",
            "1     0     0      1      1     1       0\n",
            "2     1     0      1      0     0       1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Term Frequency (TF) and inverse document frequency(IDF)**"
      ],
      "metadata": {
        "id": "Wc243CjCCYgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction with Tf-Idf vectorizer"
      ],
      "metadata": {
        "id": "A6YaArIoC_Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KYPQQO3x6QP",
        "outputId": "96dbf904-208b-4de1-9176-b96771f44597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' This movie is very scary and long',\n",
              " 'This movie is not scary and is slow',\n",
              " 'This movie is spooky and good']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "#without smooth IDF\n",
        "print(\"Without Smoothing:\")\n",
        "#define tf-idf\n",
        "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
        "                        smooth_idf=False,  \n",
        "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
        "#transform\n",
        "tf_idf_data = tf_idf_vec.fit_transform(corpus)\n",
        " \n",
        "#create dataframe\n",
        "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
        "print(tf_idf_dataframe)\n",
        "print(\"\\n\")\n",
        " \n",
        "#with smooth\n",
        "tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
        "                        smooth_idf=True,  \n",
        "                        ngram_range=(1,1),stop_words='english')\n",
        " \n",
        " \n",
        "tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform(corpus)\n",
        " \n",
        "print(\"With Smoothing:\")\n",
        "tf_idf_dataframe_smooth=pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())\n",
        "print(tf_idf_dataframe_smooth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8XXNJuJCzat",
        "outputId": "ed5db804-17ab-493f-e790-b9e9b2af7796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Smoothing:\n",
            "       good      long     movie     scary      slow    spooky\n",
            "0  0.000000  0.772536  0.368117  0.517376  0.000000  0.000000\n",
            "1  0.000000  0.000000  0.368117  0.517376  0.772536  0.000000\n",
            "2  0.670092  0.000000  0.319302  0.000000  0.000000  0.670092\n",
            "\n",
            "\n",
            "With Smoothing:\n",
            "       good      long     movie     scary      slow    spooky\n",
            "0  0.000000  0.720333  0.425441  0.547832  0.000000  0.000000\n",
            "1  0.000000  0.000000  0.425441  0.547832  0.720333  0.000000\n",
            "2  0.652491  0.000000  0.385372  0.000000  0.000000  0.652491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load text from URL\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/bhaskarfx/nlp/main/McD_Senti.csv', header = None, delimiter=',')"
      ],
      "metadata": {
        "id": "YO_KR8Tb2o6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "uk9WMCFP4ggb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47400698-c0b9-4d2e-d467-2af19894fc9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f7716d27-1ebe-4019-94bc-a0007f157ac1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>_unit_id</td>\n",
              "      <td>_golden</td>\n",
              "      <td>_unit_state</td>\n",
              "      <td>_trusted_judgments</td>\n",
              "      <td>_last_judgment_at</td>\n",
              "      <td>policies_violated</td>\n",
              "      <td>policies_violated:confidence</td>\n",
              "      <td>city</td>\n",
              "      <td>policies_violated_gold</td>\n",
              "      <td>review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>679455653</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>2/21/15 0:36</td>\n",
              "      <td>RudeService</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OrderProblem</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Filthy</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.6667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7716d27-1ebe-4019-94bc-a0007f157ac1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f7716d27-1ebe-4019-94bc-a0007f157ac1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f7716d27-1ebe-4019-94bc-a0007f157ac1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              0        1            2  ...     7                       8       9\n",
              "0      _unit_id  _golden  _unit_state  ...  city  policies_violated_gold  review\n",
              "1     679455653    False    finalized  ...   NaN                     NaN     NaN\n",
              "2  OrderProblem      NaN          NaN  ...   NaN                     NaN     NaN\n",
              "3        Filthy      1.0          NaN  ...   NaN                     NaN     NaN\n",
              "4        0.6667      NaN          NaN  ...   NaN                     NaN     NaN\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resources:**\n",
        "*  http://www.nltk.org/index.html\n",
        "*  http://textblob.readthedocs.io/en/dev/\n",
        "*  https://spacy.io/usage/facts-figures\n",
        "*  https://radimrehurek.com/gensim/index.html\n",
        "*  https://opennlp.apache.org/\n",
        "*  https://www.clips.uantwerpen.be/pages/pattern\n",
        "*  https://nlp.stanford.edu/software/tokenizer.html#About\n",
        "*  https://tartarus.org/martin/PorterStemmer/\n",
        "*  http://www.nltk.org/api/nltk.stem.html\n",
        "*  https://pypi.python.org/pypi/PyStemmer/1.0.1\n",
        "*  http://ucrel.lancs.ac.uk/claws/\n",
        "*  http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/\n",
        "*  https://en.wikipedia.org/wiki/Shallow_parsing\n",
        "*  https://www.ibm.com/support/knowledgecenter/en/SS8NLW_10.0.0/com.ibm.watson.\n",
        "*  http://www.bart-coref.org/index.html\n",
        "*  https://www.cs.utah.edu/nlp/reconcile/\n",
        "*  https://cogcomp.org/page/software_view/Coref\n",
        "*  https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/#relations"
      ],
      "metadata": {
        "id": "zYRuDMmybCSj"
      }
    }
  ]
}