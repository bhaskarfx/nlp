{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_FDP_Pre-Processing and Feature Extraction from Text Documents.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNayJIWLvvaZkX4rNF1MXGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaskarfx/nlp/blob/main/NLP_FDP_Pre_Processing_and_Feature_Extraction_from_Text_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Processing Raw Text**\n",
        "\n",
        "*by [Dr. Bhaskar Mondal](https://sites.google.com/view/bmondal/bhaskarmondal?authuser=3)*\n",
        "\n",
        "YouTube Link: https://www.youtube.com/watch?v=sQb63jW4wz4"
      ],
      "metadata": {
        "id": "-3jRVlvAfCaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R7eum49e2VW"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"You're flying on cloud no 9 :  the sky is PinkishBlue < . You shouldn't eat cardboard.\n",
        "Hello Mr. Bob, how are you doing today? The weather is greater, and city is awesome z .\"\"\""
      ],
      "metadata": {
        "id": "pJ0opg8itKvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split by Whitespace**"
      ],
      "metadata": {
        "id": "B77N8xuEs9Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=text.split()\n",
        "print(words)"
      ],
      "metadata": {
        "id": "3EObs7nis-V4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c75d995-7071-4650-f3ed-6664ae015714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"You're\", 'flying', 'on', 'cloud', 'no', '9', ':', 'the', 'sky', 'is', 'PinkishBlue', '<', '.', 'You', \"shouldn't\", 'eat', 'cardboard.', 'Hello', 'Mr.', 'Bob,', 'how', 'are', 'you', 'doing', 'today?', 'The', 'weather', 'is', 'greater,', 'and', 'city', 'is', 'awesome', 'z', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split based on word"
      ],
      "metadata": {
        "id": "r89_LeOse8bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "words1=re.split(r'\\W+', text)\n",
        "print(words1[:10])"
      ],
      "metadata": {
        "id": "m962ZvnLwBU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc1878b-e10f-49d7-d731-608b38215b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You', 're', 'flying', 'on', 'cloud', 'no', '9', 'the', 'sky', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Words and characters**"
      ],
      "metadata": {
        "id": "coH7XtuYUpPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_count=len(str(text).split(\" \"))\n",
        "word_count"
      ],
      "metadata": {
        "id": "7sgMg1c6Uvqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b307a7-a972-4d62-fb2e-fe7399a234dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_count=len(text)\n",
        "char_count"
      ],
      "metadata": {
        "id": "Dkj_B9P4VmjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b8d60c-767f-4ad0-a983-03d709c9887a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use Regular Expressions for cleaning text**\n",
        "\n"
      ],
      "metadata": {
        "id": "zAyqDlv3lRRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Attached words"
      ],
      "metadata": {
        "id": "GxQCwzJhiUnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words2=\" \".join(re.findall(\"[A-Z][^A-Z]*\", text)) #[A-C]--> A or B or C .... or Z; *--> zero or n no. of occurence \n",
        "print(len(words2))"
      ],
      "metadata": {
        "id": "H6HVW2VDkwjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211e6c19-cff7-400e-fd13-28267ef925fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Apostrophe**"
      ],
      "metadata": {
        "id": "sqty1kJnHv0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Dictionary"
      ],
      "metadata": {
        "id": "yG4PzqmblVER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "appos={\"'s\": \" is\", \"'re\" : \" are\", \"n't\": \" not\"}"
      ],
      "metadata": {
        "id": "NqYb4YpNlUnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list=text.split()\n",
        "\n",
        "new_sent=[]\n",
        "\n",
        "for word in word_list:\n",
        "  for key in appos:\n",
        "    word=word.replace(key, appos[key])\n",
        "  new_sent.append(word)\n",
        "\n",
        "new_text=\" \".join(new_sent)\n",
        "print(new_text)"
      ],
      "metadata": {
        "id": "zqZvJx-pH6mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b978e6c2-60a1-403d-9f88-8c05b1f75b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are flying on cloud no 9 : the sky is PinkishBlue < . You should not eat cardboard. Hello Mr. Bob, how are you doing today? The weather is greater, and city is awesome z .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Reg. Exp."
      ],
      "metadata": {
        "id": "35tcNMm0osGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decontraction(phrase):\n",
        "  phrase=re.sub(r\"\\'re\", \" are\", phrase)\n",
        "  phrase=re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "\n",
        "  return phrase\n",
        "\n",
        "new_text2=decontraction(text)\n",
        "print(new_text)"
      ],
      "metadata": {
        "id": "V68zfH5HYTaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568a5053-2556-4f1b-8aa4-e83a0dc5c630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are flying on cloud no 9 : the sky is PinkishBlue < . You should not eat cardboard. Hello Mr. Bob, how are you doing today? The weather is greater, and city is awesome z .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Python Library"
      ],
      "metadata": {
        "id": "X6NaKFVKqGSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj9U_dzwqW-W",
        "outputId": "7e9dffa7-298f-4272-a140-a98670f96db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 15.7 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 62.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85450 sha256=bc81c2904ab20925d9b18e568809978f202dc2aac8921e304c98ffd56a3d5286\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions"
      ],
      "metadata": {
        "id": "qKzEO0AQqBj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expan_words=[]\n",
        "for word in words:\n",
        "  #text.split()\n",
        "  expan_words.append(contractions.fix(word))\n",
        "\n",
        "new_text=\" \".join(expan_words)\n",
        "print(new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dzBD0upqj1k",
        "outputId": "1e19aa1c-f7bb-4d0a-e8f8-88a80d36db57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you are flying on cloud no 9 : the sky is pinkishblue < . you should not eat cardboard. hello mr. bob, how are you doing today? the weather is greater, and city is awesome z .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average Word Length**"
      ],
      "metadata": {
        "id": "GzE0OKa4WozW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_word_len(sen):\n",
        "  words=sen.split(\" \")\n",
        "  total_len=sum(len(word) for word in words)\n",
        "  no_of_words=len(words)\n",
        "  return (total_len/no_of_words)\n",
        "\n",
        "avg_len=avg_word_len(text)\n",
        "print(avg_len)"
      ],
      "metadata": {
        "id": "PDajEkPkWnUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0698cc2c-536b-4256-f362-a1b74f54e5f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Single Chars"
      ],
      "metadata": {
        "id": "ZcsQ3gr8seSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_single_char(sen):\n",
        "  new_text=\"\"\n",
        "  words=sen.split()\n",
        "  for word in words:\n",
        "    if len(word)>1:\n",
        "      new_text=new_text + \" \" + word\n",
        "  return new_text\n",
        "\n",
        "new_text=remove_single_char(text)\n",
        "print(new_text)"
      ],
      "metadata": {
        "id": "-1VRHo5cZHmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907b16c7-bcba-48d8-a487-f37a78300c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " You're flying on cloud no the sky is PinkishBlue You shouldn't eat cardboard. Hello Mr. Bob, how are you doing today? The weather is greater, and city is awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text=\" \".join([w for w in text.split() if len(w)>1])\n",
        "new_text"
      ],
      "metadata": {
        "id": "sqFVqxjiWzHJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a35bcda1-c736-4440-afe2-09e1200a4ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"You're flying on cloud no the sky is PinkishBlue You shouldn't eat cardboard. Hello Mr. Bob, how are you doing today? The weather is greater, and city is awesome\""
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizing Case:** **convert to lower case** "
      ],
      "metadata": {
        "id": "5qFL0Nprx3uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sky, SKY, skY\n",
        "def convert_to_lower(words):\n",
        "  words=[word.lower() for word in words]\n",
        "  return words\n",
        "\n",
        "in_lower=convert_to_lower(words)\n",
        "print(in_lower)"
      ],
      "metadata": {
        "id": "R0hEWz9qx6kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d1a615-6a32-4636-c4f9-b1c00110bee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"you're\", 'flying', 'on', 'cloud', 'no', '9', ':', 'the', 'sky', 'is', 'pinkishblue', '<', '.', 'you', \"shouldn't\", 'eat', 'cardboard.', 'hello', 'mr.', 'bob,', 'how', 'are', 'you', 'doing', 'today?', 'the', 'weather', 'is', 'greater,', 'and', 'city', 'is', 'awesome', 'z', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Stop Words**"
      ],
      "metadata": {
        "id": "XkUfoVKW8YI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "lqmdKyRJ8gTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a87ea31-f92f-4a98-acf3-0e5870e61608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_eng=stopwords.words('english')\n",
        "print(stop_eng)\n",
        "print(len(stop_eng))"
      ],
      "metadata": {
        "id": "0VPU5LB68Y5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0a38a0-5c5b-43f2-f0ff-9150f406156d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count no. of stopwords"
      ],
      "metadata": {
        "id": "MSDtxgeRwUaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_stop_words(sen):\n",
        "  words=sen.split()\n",
        "  my_stop=[]\n",
        "  for w in words:\n",
        "    if w in stop_eng:\n",
        "      my_stop.append(w)\n",
        "  return len(my_stop)\n",
        "\n",
        "total_no_stops=count_stop_words(text)\n",
        "print(total_no_stops)"
      ],
      "metadata": {
        "id": "blUU1teDXtEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55bc4a50-5650-44cb-df7c-33381a117b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_removed=[w for w in convert_to_lower(new_text2.split()) if not w in stop_eng]\n",
        "print(stop_removed)"
      ],
      "metadata": {
        "id": "foLsj44n9Jm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652bff94-7275-47ae-eca4-e6ba8cebb6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['flying', 'cloud', '9', ':', 'sky', 'pinkishblue', '<', '.', 'eat', 'cardboard.', 'hello', 'mr.', 'bob,', 'today?', 'weather', 'greater,', 'city', 'awesome', 'z', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(lower_tokens):\n",
        "    filtered_words=[]\n",
        "    for word in lower_tokens:\n",
        "      if word not in stop_eng:\n",
        "        filtered_words.append(word)\n",
        "    return filtered_words\n",
        "\n",
        "print(remove_stopwords(words))"
      ],
      "metadata": {
        "id": "iAv564-KJ_CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486223db-19d3-43f0-d7e2-633a8d58cc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['flying', 'cloud', '9', ':', 'sky', 'pinkishblue', '<', '.', 'eat', 'cardboard.', 'hello', 'mr.', 'bob,', 'today?', 'weather', 'greater,', 'city', 'awesome', 'z', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Punctuation**"
      ],
      "metadata": {
        "id": "-4NfN-fs8QhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "Mdzbajv9wXMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.punctuation)"
      ],
      "metadata": {
        "id": "d1VTXMxlwiS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e065fc27-4f8f-4b4c-c933-9dd50e5ea5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(words):\n",
        "  table=str.maketrans('', '', string.punctuation)\n",
        "  stripped=[w.translate(table) for w in words]\n",
        "  return stripped\n",
        "\n",
        "\n",
        "words=convert_to_lower(new_text2.split())\n",
        "punc_removed=remove_punc(words)\n",
        "print(punc_removed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bMvf5fhy5Ir",
        "outputId": "ae504365-f738-491c-89f3-079015eac04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you', 'are', 'flying', 'on', 'cloud', 'no', '9', '', 'the', 'sky', 'is', 'pinkishblue', '', '', 'you', 'should', 'not', 'eat', 'cardboard', 'hello', 'mr', 'bob', 'how', 'are', 'you', 'doing', 'today', 'the', 'weather', 'is', 'greater', 'and', 'city', 'is', 'awesome', 'z', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of numerics**"
      ],
      "metadata": {
        "id": "C56IdIUWb5JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_of_num(sen):\n",
        "  words=convert_to_lower(sen.split())\n",
        "  count=0\n",
        "  for w in words:\n",
        "    if w.isdigit():\n",
        "      count+=1\n",
        "  return count\n",
        "\n",
        "no_of_num=count_of_num(new_text2)\n",
        "print(no_of_num)"
      ],
      "metadata": {
        "id": "c-FCMc_Nb5cn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186f2651-b36a-448f-df32-3896854d36c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove numerics "
      ],
      "metadata": {
        "id": "5CunOc0-iRgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_nums(words):\n",
        "  new_string = ''.join((x for x in words if not x.isdigit()))\n",
        "  return new_string\n",
        "\n",
        "print(remove_nums(text))"
      ],
      "metadata": {
        "id": "bEMFRBr8Ls0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368bc33e-3f8f-4885-c092-5858e93263c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're flying on cloud no  :  the sky is PinkishBlue < . You shouldn't eat cardboard.\n",
            "Hello Mr. Bob, how are you doing today? The weather is greater, and city is awesome z .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**"
      ],
      "metadata": {
        "id": "5493ZFydgUpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split into Sentences**"
      ],
      "metadata": {
        "id": "Ggy5S4w55--0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "0n7bNVrU6GJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d026445-6d46-4931-fb0a-fbef8b717430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "g3FDRiIi52_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74e6d29-adc1-43d6-a105-d5a29634c46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"You're flying on cloud no 9 :  the sky is PinkishBlue < .\", \"You shouldn't eat cardboard.\", 'Hello Mr. Bob, how are you doing today?', 'The weather is greater, and city is awesome z .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "split into words"
      ],
      "metadata": {
        "id": "CV_NA3AlsUqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(sentence):\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  return (word_tokenize(sentence))\n",
        "\n",
        "tokens=tokenization(new_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "yVz6YHqh7Irk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173e3860-64b7-4e94-e259-1889eecdd1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you', 'are', 'flying', 'on', 'cloud', 'no', '9', ':', 'the', 'sky', 'is', 'pinkishblue', '<', '.', 'you', 'should', 'not', 'eat', 'cardboard', '.', 'hello', 'mr.', 'bob', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'the', 'weather', 'is', 'greater', ',', 'and', 'city', 'is', 'awesome', 'z', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "TextBlob(new_text).words"
      ],
      "metadata": {
        "id": "11Du1PAggmqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de22020f-8d88-4878-f975-42fad218e8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['you', 'are', 'flying', 'on', 'cloud', 'no', '9', 'the', 'sky', 'is', 'pinkishblue', 'you', 'should', 'not', 'eat', 'cardboard', 'hello', 'mr', 'bob', 'how', 'are', 'you', 'doing', 'today', 'the', 'weather', 'is', 'greater', 'and', 'city', 'is', 'awesome', 'z'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spelling correction**"
      ],
      "metadata": {
        "id": "fBhXdZW7dryL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_correct(sentence):\n",
        "  from textblob import TextBlob\n",
        "  sentence = TextBlob(sentence).correct()\n",
        "  return sentence\n",
        "\n",
        "no_of_nums=spell_correct(new_text)\n",
        "no_of_nums"
      ],
      "metadata": {
        "id": "eUH9WODjdsiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da868b6-3e37-42ae-b570-0445b7a841a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"you are flying on cloud no 9 : the sky is pinkishblue < . you should not eat cardboard. hello mr. bob, how are you doing today? the weather is greater, and city is awesome z .\")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming and Lemmatization**\n",
        "\n",
        "\n",
        "difference is that *stem may not be an actual word* whereas, *lemma is an actual language word*.\n",
        "\n",
        "Stemming follows an algorithm with steps to perform on the words which makes it faster. \n",
        "\n",
        "Lemmatization use a corpus also to supply lemma which makes it slower than stemming. you furthermore might had to define a parts-of-speech to get the proper lemma."
      ],
      "metadata": {
        "id": "YyiQAdFaTZ77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "\n",
        "#proide a word to be stemmed\n",
        "print(\"Porter Stemmer\")\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"trouble\"))\n",
        "\n",
        "print(\"Lancaster Stemmer\") \n",
        "print(lancaster.stem(\"cats\"))"
      ],
      "metadata": {
        "id": "Sww6cHykTaJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211e4847-892f-48ce-b8f7-4b56252b4c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer\n",
            "cat\n",
            "troubl\n",
            "Lancaster Stemmer\n",
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stem Words**"
      ],
      "metadata": {
        "id": "MUVuvcVZ9kjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in words]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "id": "LQOZreKZ9lvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dfaccd-d8a5-4f23-f876-ab52867c6395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you', 'are', 'fli', 'on', 'cloud', 'no', '9', ':', 'the', 'sky', 'is', 'pinkishblu', '<', '.', 'you', 'should', 'not', 'eat', 'cardboard.', 'hello', 'mr.', 'bob,', 'how', 'are', 'you', 'do', 'today?', 'the', 'weather', 'is', 'greater,', 'and', 'citi', 'is', 'awesom', 'z', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization** "
      ],
      "metadata": {
        "id": "KVOtLyz_-95-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei1o0DOSuPIo",
        "outputId": "ee707a38-7e93-445b-d9c3-2c24a75745ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "\n",
        "def lemmatization(sentence):\n",
        "  words = sentence.split()\n",
        "  lemmas=[]\n",
        "  for w in words:\n",
        "      lemmas.append(Word(w).lemmatize())\n",
        "  return lemmas\n",
        "\n",
        "lemmas=lemmatization(text)\n",
        "print(lemmas)"
      ],
      "metadata": {
        "id": "gtId6-va_BSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d37dcb6-ee8f-4075-d8e9-5dfbeb9c2b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"You're\", 'flying', 'on', 'cloud', 'no', '9', ':', 'the', 'sky', 'is', 'PinkishBlue', '<', '.', 'You', \"shouldn't\", 'eat', 'cardboard.', 'Hello', 'Mr.', 'Bob,', 'how', 'are', 'you', 'doing', 'today?', 'The', 'weather', 'is', 'greater,', 'and', 'city', 'is', 'awesome', 'z', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of speech tagging (POS)**"
      ],
      "metadata": {
        "id": "JlP1zH_0EVxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "VrnwYT93Y30M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2dded45-bc5a-431d-f275-75f2a955ade4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "result = TextBlob(new_text)\n",
        "print(result.tags)"
      ],
      "metadata": {
        "id": "yNik-6rIXuPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8ab466-41e1-4ab1-8c33-3391a5de38fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('you', 'PRP'), ('are', 'VBP'), ('flying', 'VBG'), ('on', 'IN'), ('cloud', 'NN'), ('no', 'DT'), ('9', 'CD'), ('the', 'DT'), ('sky', 'NN'), ('is', 'VBZ'), ('pinkishblue', 'JJ'), ('<', 'NN'), ('you', 'PRP'), ('should', 'MD'), ('not', 'RB'), ('eat', 'VB'), ('cardboard', 'NN'), ('hello', 'NN'), ('mr.', 'NN'), ('bob', 'NN'), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('today', 'NN'), ('the', 'DT'), ('weather', 'NN'), ('is', 'VBZ'), ('greater', 'JJR'), ('and', 'CC'), ('city', 'NN'), ('is', 'VBZ'), ('awesome', 'JJ'), ('z', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking using NLTK:**\n",
        "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.)"
      ],
      "metadata": {
        "id": "xd8XcJXvYPwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(new_text)\n",
        "print(tokens)\n",
        "\n",
        "tag = nltk.pos_tag(tokens)\n",
        "print(tag)\n",
        "\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp  =nltk.RegexpParser(grammar)\n",
        "result = cp.parse(tag)\n",
        "print(result)\n",
        "# result.draw()"
      ],
      "metadata": {
        "id": "v6PLlyFkZKPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf027602-81ef-46c6-b902-c884694b4d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you', 'are', 'flying', 'on', 'cloud', 'no', '9', ':', 'the', 'sky', 'is', 'pinkishblue', '<', '.', 'you', 'should', 'not', 'eat', 'cardboard', '.', 'hello', 'mr.', 'bob', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'the', 'weather', 'is', 'greater', ',', 'and', 'city', 'is', 'awesome', 'z', '.']\n",
            "[('you', 'PRP'), ('are', 'VBP'), ('flying', 'VBG'), ('on', 'IN'), ('cloud', 'NN'), ('no', 'DT'), ('9', 'CD'), (':', ':'), ('the', 'DT'), ('sky', 'NN'), ('is', 'VBZ'), ('pinkishblue', 'JJ'), ('<', 'NNP'), ('.', '.'), ('you', 'PRP'), ('should', 'MD'), ('not', 'RB'), ('eat', 'VB'), ('cardboard', 'NN'), ('.', '.'), ('hello', 'VB'), ('mr.', 'JJ'), ('bob', 'NN'), (',', ','), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('today', 'NN'), ('?', '.'), ('the', 'DT'), ('weather', 'NN'), ('is', 'VBZ'), ('greater', 'JJR'), (',', ','), ('and', 'CC'), ('city', 'NN'), ('is', 'VBZ'), ('awesome', 'JJ'), ('z', 'NN'), ('.', '.')]\n",
            "(S\n",
            "  you/PRP\n",
            "  are/VBP\n",
            "  flying/VBG\n",
            "  on/IN\n",
            "  (NP cloud/NN)\n",
            "  no/DT\n",
            "  9/CD\n",
            "  :/:\n",
            "  (NP the/DT sky/NN)\n",
            "  is/VBZ\n",
            "  pinkishblue/JJ\n",
            "  </NNP\n",
            "  ./.\n",
            "  you/PRP\n",
            "  should/MD\n",
            "  not/RB\n",
            "  eat/VB\n",
            "  (NP cardboard/NN)\n",
            "  ./.\n",
            "  hello/VB\n",
            "  (NP mr./JJ bob/NN)\n",
            "  ,/,\n",
            "  how/WRB\n",
            "  are/VBP\n",
            "  you/PRP\n",
            "  doing/VBG\n",
            "  (NP today/NN)\n",
            "  ?/.\n",
            "  (NP the/DT weather/NN)\n",
            "  is/VBZ\n",
            "  greater/JJR\n",
            "  ,/,\n",
            "  and/CC\n",
            "  (NP city/NN)\n",
            "  is/VBZ\n",
            "  (NP awesome/JJ z/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "60dpuDA1aLf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hl_l5NC0EWBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7uN3EOK_AhIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advance Text Processing: N-grams**"
      ],
      "metadata": {
        "id": "adPkhsP5iqtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(new_text).ngrams(3)"
      ],
      "metadata": {
        "id": "8CpaCpKQirGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a61afa-37a1-4ae7-9ab2-91da384b2313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['you', 'are', 'flying']),\n",
              " WordList(['are', 'flying', 'on']),\n",
              " WordList(['flying', 'on', 'cloud']),\n",
              " WordList(['on', 'cloud', 'no']),\n",
              " WordList(['cloud', 'no', '9']),\n",
              " WordList(['no', '9', 'the']),\n",
              " WordList(['9', 'the', 'sky']),\n",
              " WordList(['the', 'sky', 'is']),\n",
              " WordList(['sky', 'is', 'pinkishblue']),\n",
              " WordList(['is', 'pinkishblue', 'you']),\n",
              " WordList(['pinkishblue', 'you', 'should']),\n",
              " WordList(['you', 'should', 'not']),\n",
              " WordList(['should', 'not', 'eat']),\n",
              " WordList(['not', 'eat', 'cardboard']),\n",
              " WordList(['eat', 'cardboard', 'hello']),\n",
              " WordList(['cardboard', 'hello', 'mr']),\n",
              " WordList(['hello', 'mr', 'bob']),\n",
              " WordList(['mr', 'bob', 'how']),\n",
              " WordList(['bob', 'how', 'are']),\n",
              " WordList(['how', 'are', 'you']),\n",
              " WordList(['are', 'you', 'doing']),\n",
              " WordList(['you', 'doing', 'today']),\n",
              " WordList(['doing', 'today', 'the']),\n",
              " WordList(['today', 'the', 'weather']),\n",
              " WordList(['the', 'weather', 'is']),\n",
              " WordList(['weather', 'is', 'greater']),\n",
              " WordList(['is', 'greater', 'and']),\n",
              " WordList(['greater', 'and', 'city']),\n",
              " WordList(['and', 'city', 'is']),\n",
              " WordList(['city', 'is', 'awesome']),\n",
              " WordList(['is', 'awesome', 'z'])]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Bag-of-Words (BoW) and TF-IDF for Creating Features from Text**\n",
        "Lets have 3 review:\n",
        "\n",
        "\n",
        "*   Review 1: This movie is very scary and long\n",
        "*   Review 2: This movie is not scary and is slow\n",
        "*   Review 3: This movie is spooky and good\n",
        "\n",
        "The vocabulary consists of these 11 words:\n",
        "\\begin{array}{c|c|c|c|c|c|c|c|c|c|c|c}\\hline\n",
        "&This& movie& is& very& scary& and& long& not&  slow& spooky&  good& Review Length\\\\\\hline\n",
        "Review 1&1&1&1&1&1&1&1&0&0&0&0&7\\\\\\hline\n",
        "Review 2&1&1&2&0&0&1&1&0&1&0&0&8\\\\\\hline\n",
        "Review 3&1&1&1&0&0&0&1&0&0&1&1&6\\\\\\hline\n",
        "\\end{array}"
      ],
      "metadata": {
        "id": "2VjTdCDGwQtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Term Frequency-Inverse Document Frequency**\n",
        "---\n",
        "*   **Term Frequency (TF):** In document d, the frequency represents the number of instances of a given word/ term t.\n",
        "\n",
        ">> $tf(t,d) = \\frac{\\textit{count of t in d} }{ \\textit{number of terms in d}}$\n",
        "\n",
        ">>for example \n",
        "* TF(‘movie’) = 1/8\n",
        "* TF(‘is’) = 2/8 = 1/4\n",
        "\n",
        "*   **Document Frequency (DF):** is the number of total occurrences of the term $t$ in the document set N (entire corpus).\n",
        "\n",
        ">>$df(t) =$ occurrence of $t$ in all documents\n",
        "\n",
        "\n",
        "\n",
        "* **Inverse Document Frequency (IDF):** Log of the ratio numbers of document $N$ and $df(t)$\n",
        "\n",
        ">>$idf(t) = \\log(\\frac{N}{ df(t)})$\n",
        "\n",
        ">>for example \n",
        "* IDF(‘movie’, ) = log(3/3) = 0\n",
        "* IDF(‘is’) = log(3/3) = 0\n",
        "* IDF(‘not’) = log(3/1) = log(3) = 0.48\n",
        "\n",
        "* **Computation of tf-idf weight:** \n",
        ">> tf-idf$(t, d) = tf(t, d) \\times idf(t)$\n",
        "\n",
        ">> for example \n",
        "* TF-IDF(‘this’, Review 2) = TF(‘this’, Review 2) * IDF(‘this’) = 1/8 * 0 = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "8XReeA_yjGpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r1=\" This movie is very scary and long\"\n",
        "r2=\"This movie is not scary and is slow\"\n",
        "r3=\"This movie is spooky and good\"\n",
        "corpus=[r1, r2, r3]\n",
        "corpus"
      ],
      "metadata": {
        "id": "2d0p56HDBj7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59cd940b-facb-48c9-d8c8-eb60fb6f0bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' This movie is very scary and long',\n",
              " 'This movie is not scary and is slow',\n",
              " 'This movie is spooky and good']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  preprocessed=[]\n",
        "  \n",
        "  for d in data:\n",
        "    tokens=tokenization(d)\n",
        "    tokens = convert_to_lower(tokens)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = remove_punc(tokens)\n",
        "    preprocessed.append(tokens)\n",
        "  return preprocessed\n",
        "preprocessed=preprocess(corpus)\n",
        "print(preprocessed)"
      ],
      "metadata": {
        "id": "PBr4jKXxfICI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043ffebb-cc57-4c0f-8c35-313277c4a01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['movie', 'scary', 'long'], ['movie', 'scary', 'slow'], ['movie', 'spooky', 'good']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a vocabulary list\n",
        "set().union(*preprocessed)"
      ],
      "metadata": {
        "id": "k5N7buDjfqOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a29b10a-3af0-4b29-ba64-acf04f9fe6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'good', 'long', 'movie', 'scary', 'slow', 'spooky'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a vocabulary list\n",
        "def vocab_list(ListofList):\n",
        "  list_new=[]\n",
        "  cnt=0\n",
        "  while cnt<len(ListofList):\n",
        "    for i in ListofList[cnt]:\n",
        "        if i in list_new:\n",
        "            continue\n",
        "        else:\n",
        "            list_new.append(i)\n",
        "    cnt+=1\n",
        "  return list_new\n",
        "vocab=vocab_list(preprocessed)\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "CFZUSu44fy_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed27259c-0ae8-4248-ac2e-5ba4a7f009d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie', 'scary', 'long', 'slow', 'spooky', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Term frequency\n",
        "def tf(corpus):\n",
        "    dic={}\n",
        "    for document in corpus:\n",
        "        for word in document.split():\n",
        "            if word in dic:\n",
        "                dic[word] = dic[word] + 1\n",
        "            else:\n",
        "                dic[word]=1\n",
        "    for word,freq in dic.items():\n",
        "        print(word,freq)\n",
        "        dic[word]=freq/sum(map(len, (document.split() for document in corpus)))\n",
        "    return dic\n",
        "tf=tf(corpus)\n",
        "print(tf)"
      ],
      "metadata": {
        "id": "_f6BbDAUf9Tk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c744f7c-7aa1-4ce6-9421-4c5e6b8bf9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This 3\n",
            "movie 3\n",
            "is 4\n",
            "very 1\n",
            "scary 2\n",
            "and 3\n",
            "long 1\n",
            "not 1\n",
            "slow 1\n",
            "spooky 1\n",
            "good 1\n",
            "{'This': 0.14285714285714285, 'movie': 0.14285714285714285, 'is': 0.19047619047619047, 'very': 0.047619047619047616, 'scary': 0.09523809523809523, 'and': 0.14285714285714285, 'long': 0.047619047619047616, 'not': 0.047619047619047616, 'slow': 0.047619047619047616, 'spooky': 0.047619047619047616, 'good': 0.047619047619047616}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Inverse Document Frequency\n",
        "import math\n",
        "import numpy as np\n",
        "def IDF(corpus, vocab):\n",
        "  idf_dict={}\n",
        "  N=len(corpus)\n",
        "  for i in vocab:\n",
        "    count=0\n",
        "    for sen in corpus:\n",
        "      if i in sen.split():\n",
        "        count=count+1\n",
        "        idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
        "  return idf_dict"
      ],
      "metadata": {
        "id": "6n1xuuRdjJmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf=IDF(corpus, vocab)\n",
        "print(idf)"
      ],
      "metadata": {
        "id": "e6pJOWeyjcXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728c76c9-7615-4245-9ea5-d71830c06a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'movie': 1.0, 'scary': 1.2876820724517808, 'long': 1.6931471805599454, 'slow': 1.6931471805599454, 'spooky': 1.6931471805599454, 'good': 1.6931471805599454}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Term Frequency – Inverse Document Frequency (TF-IDF)\n",
        "tfidf={}\n",
        "\n",
        "res = {key: tf[key] * idf.get(key, 0) \n",
        "                       for key in tf.keys()}\n",
        "\n",
        "res"
      ],
      "metadata": {
        "id": "AXoj6sGIjiHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17a5fd7-0e07-483c-86a5-0d19dba252f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'This': 0.0,\n",
              " 'and': 0.0,\n",
              " 'good': 0.08062605621714025,\n",
              " 'is': 0.0,\n",
              " 'long': 0.08062605621714025,\n",
              " 'movie': 0.14285714285714285,\n",
              " 'not': 0.0,\n",
              " 'scary': 0.12263638785255054,\n",
              " 'slow': 0.08062605621714025,\n",
              " 'spooky': 0.08062605621714025,\n",
              " 'very': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "KczGhpkQqWLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WU299MzbqMTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature extraction: Bag of Words**"
      ],
      "metadata": {
        "id": "lKK_cNg__G2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Bag of Words Algorithm with Python**"
      ],
      "metadata": {
        "id": "XOuoX0pRBgvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_list(ListofList):\n",
        "  list_new=[]\n",
        "  cnt=0\n",
        "  while cnt<len(ListofList):\n",
        "    for i in ListofList[cnt]:\n",
        "        if i in list_new:\n",
        "            continue\n",
        "        else:\n",
        "            list_new.append(i)\n",
        "    cnt+=1\n",
        "  return list_new\n",
        "vocab_l=vocab_list(preprocessed)\n",
        "print(vocab_l)"
      ],
      "metadata": {
        "id": "UjdmAP-cp4Bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e5a752-91c9-4fe3-9fc0-7baf9db90cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie', 'scary', 'long', 'slow', 'spooky', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens):\n",
        "    vocab=vocab_list(tokens)\n",
        "\n",
        "\n",
        "    vectors=[]\n",
        "    for t in tokens:\n",
        "      print(t)\n",
        "      vector=[]\n",
        "      for w in vocab:\n",
        "        vector.append(t.count(w))\n",
        "      # print(vector)\n",
        "      vectors.append(vector)\n",
        "    return vectors"
      ],
      "metadata": {
        "id": "GYc41z43_Or-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors=vectorize(preprocessed)\n",
        "print(vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDZbpNwf2RuW",
        "outputId": "85052764-a929-4fe6-b1ed-b8b02321829b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie', 'scary', 'long']\n",
            "['movie', 'scary', 'slow']\n",
            "['movie', 'spooky', 'good']\n",
            "[[1, 1, 1, 0, 0, 0], [1, 1, 0, 1, 0, 0], [1, 0, 0, 0, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Bag of Words Model with Sklearn**"
      ],
      "metadata": {
        "id": "9NsehhSOB1CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
        "                           stop_words='english')\n",
        "#transform\n",
        "Count_data = CountVec.fit_transform(corpus)\n",
        " \n",
        "#create dataframe\n",
        "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
        "print(cv_dataframe)"
      ],
      "metadata": {
        "id": "A4WOuJyYByVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a8a3af-65f4-433f-d63b-108ba2420d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   good  long  movie  scary  slow  spooky\n",
            "0     0     1      1      1     0       0\n",
            "1     0     0      1      1     1       0\n",
            "2     1     0      1      0     0       1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-Grams?**"
      ],
      "metadata": {
        "id": "9xtp5w-bCEOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RcVucOauCIa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Term Frequency (TF) and inverse document frequency(IDF)**"
      ],
      "metadata": {
        "id": "Wc243CjCCYgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction with Tf-Idf vectorizer"
      ],
      "metadata": {
        "id": "A6YaArIoC_Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "#without smooth IDF\n",
        "print(\"Without Smoothing:\")\n",
        "#define tf-idf\n",
        "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
        "                        smooth_idf=False,  \n",
        "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
        "#transform\n",
        "tf_idf_data = tf_idf_vec.fit_transform(corpus)\n",
        " \n",
        "#create dataframe\n",
        "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
        "print(tf_idf_dataframe)\n",
        "print(\"\\n\")\n",
        " \n",
        "#with smooth\n",
        "tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
        "                        smooth_idf=True,  \n",
        "                        ngram_range=(1,1),stop_words='english')\n",
        " \n",
        " \n",
        "tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform(corpus)\n",
        " \n",
        "print(\"With Smoothing:\")\n",
        "tf_idf_dataframe_smooth=pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())\n",
        "print(tf_idf_dataframe_smooth)"
      ],
      "metadata": {
        "id": "5KYPQQO3x6QP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a41cc0d-cd38-4ed5-eefc-5580b93caafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Smoothing:\n",
            "       good      long     movie     scary      slow    spooky\n",
            "0  0.000000  0.772536  0.368117  0.517376  0.000000  0.000000\n",
            "1  0.000000  0.000000  0.368117  0.517376  0.772536  0.000000\n",
            "2  0.670092  0.000000  0.319302  0.000000  0.000000  0.670092\n",
            "\n",
            "\n",
            "With Smoothing:\n",
            "       good      long     movie     scary      slow    spooky\n",
            "0  0.000000  0.720333  0.425441  0.547832  0.000000  0.000000\n",
            "1  0.000000  0.000000  0.425441  0.547832  0.720333  0.000000\n",
            "2  0.652491  0.000000  0.385372  0.000000  0.000000  0.652491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m8XXNJuJCzat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YO_KR8Tb2o6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uk9WMCFP4ggb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoding the text**"
      ],
      "metadata": {
        "id": "1BisKCfliIj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vXZ3_7LOiGD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4-GtwPflkQJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named entity recognition**\n",
        "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)."
      ],
      "metadata": {
        "id": "t9nsto9QaOSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resources:**\n",
        "*  http://www.nltk.org/index.html\n",
        "*  http://textblob.readthedocs.io/en/dev/\n",
        "*  https://spacy.io/usage/facts-figures\n",
        "*  https://radimrehurek.com/gensim/index.html\n",
        "*  https://opennlp.apache.org/\n",
        "*  https://www.clips.uantwerpen.be/pages/pattern\n",
        "*  https://nlp.stanford.edu/software/tokenizer.html#About\n",
        "*  https://tartarus.org/martin/PorterStemmer/\n",
        "*  http://www.nltk.org/api/nltk.stem.html\n",
        "*  https://pypi.python.org/pypi/PyStemmer/1.0.1\n",
        "*  http://ucrel.lancs.ac.uk/claws/\n",
        "*  http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/\n",
        "*  https://en.wikipedia.org/wiki/Shallow_parsing\n",
        "*  https://www.ibm.com/support/knowledgecenter/en/SS8NLW_10.0.0/com.ibm.watson.\n",
        "*  http://www.bart-coref.org/index.html\n",
        "*  https://www.cs.utah.edu/nlp/reconcile/\n",
        "*  https://cogcomp.org/page/software_view/Coref\n",
        "*  https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/#relations\n",
        "* PoS Tagging https://www.guru99.com/pos-tagging-chunking-nltk.html"
      ],
      "metadata": {
        "id": "zYRuDMmybCSj"
      }
    }
  ]
}